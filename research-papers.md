# Generative AI Research Papers Repository

Welcome to the Generative AI Research Papers Repository. This repository serves as a curated collection of influential and emerging research in the field of Generative AI. 

The table below organizes research papers into four key columns:

1. **Topic**: The specific area or subdomain of Generative AI.
2. **Paper Title**: The title of the research paper, linked to the source or publication.
3. **Summary**: A brief overview of the paper's contributions and findings.
4. **Article**: AI Generated Article based on the research paper.
## GenAI Research

| Topic               | Paper Title                                                   | Summary                                                                                                                                                                    | Article                                                                                         |
|---------------------|---------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|
| **JAIL BREAKING**    | [Many Shot Jailbreaking](https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf) | The study explores long-context attacks on large language models, using hundreds of demonstrations of undesirable behavior, made possible by recent larger context windows. The results show that these attacks follow a power law, significantly impacting state-of-the-art models and revealing new vulnerabilities. | [view article](https://github.com/manideep-malyala/genai-research/blob/main/articles/many-shot-jail-breaking.md) |
| **LARGE CONCEPT MODEL** | [Large Concept Models](https://arxiv.org/pdf/2412.08821v2)       | This paper introduces the Large Concept Model (LCM), which leverages higher-level semantic representations, or "concepts," to perform autoregressive sentence prediction. The model, trained on a massive dataset, demonstrates strong performance in tasks such as summarization and summary expansion, achieving impressive zero-shot generalization across multiple languages. | [view article]() |
| **PROMPT ENGINEERING** | [Large Language Models are Zero Short Reasonsers](https://arxiv.org/pdf/2205.11916) | Zero-shot Chain of Thought (CoT) is a simple yet powerful prompting method that enhances multi-step reasoning in large language models by using a universal prompt like "Let’s think step by step." It eliminates the need for task-specific examples, making it efficient and adaptable across diverse tasks. | [view article](articles/zero-shot-cot.md) |
| **PROMPT ENGINEERING** | [Take a Step Back: Evoking Reasoning Via Abstraction in Large Language Models](https://arxiv.org/pdf/2310.06117) | Step-Back Prompting is a technique that guides large language models (LLMs) to first perform abstraction by deriving high-level concepts or principles before engaging in detailed reasoning. This two-step process—abstraction followed by reasoning—helps reduce errors in intermediate steps and improves the model's ability to solve complex, reasoning-intensive tasks. | [ view article ]()
